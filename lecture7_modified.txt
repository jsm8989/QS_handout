Starting off with a nice experimental section first. This picture you should recognise from last time or something, very much like it, it is the familiar superfluid to Mott insulator transition. 
[pic]
And you can see that here we have sharp superfluid peaks that are characteristic of the superfluid in time of flight. The interference peaks that arise from the coherence that the superfluid can have between lattice sites. In contrast with the Mott insulating phase that is much more blurry and smeared out and reflects the loss of that coherence between sites. And we probably haven't seen this picture arrayed in a in a grid - You've probably just seen one of these rows as a function of the lattice depth? The parameters that you've seen that you can tune in the Hamiltonian are the on site energy and the hopping terms. And it's the interplay between these that determines whether you're in the Mott insulating or the superfluid regime. And that red line indicates roughly that cut-off. But experimentally, you can't directly tune on site interaction and hopping so directly, so we rely on the experimental parameters that we can control: the the power of our optical lattice beams, that determines the the lattice depth which is shown here in units of the recoil energy. And we can also apply a magnetic field to tune the scattering length that controls the strength of the interactions, so we have short scattering length (essentially a non interacting system) versus very strong interactions with a much longer scattering length. And these affect those $U/J$ terms, so for instance, increasing that depth will increase the strength of the on site interactions slightly, but it will also substantially reduce the tunnelling between the sites as the atoms become more localised to a single site. And and so you can see that the combination of these parameters can affect. The point at which that transition between the Mott insulator and the superfluid occurs. Those are the kinds of experimental parameters that we have access to. 

In terms of actually creating a system, let's look now at some measurements that you can make. We've got here some in-situ images. I think most of the images you've probably seen so far have been time of flight images. 
[pic]
Here we've got a system that has been very, very carefully loaded, one layer of the optical lattice. And you can then image that directly from above. And what we have here is moving from left to right with increasing lattice depth. That's the same as saying we're moving from the superfluid towards the Mott insulating regime. And what you can see very clearly here is that the actual density in the optical lattice starts to create quite a strong plateau. Once you're in the insulating regime so that dense density is being, this is an $n=1$ MI. And so this is being pinned to a density of $n=1$ one in the centre. Remember that you've got the harmonic confinement underlying this, so you're varying chemical potential, giving you the the superfluid outer shell. And you can see this in these line plots, too, so this is extracted
[pic]
from these images, where you can see that the Mott insulator, the red line up here is is pinned to that $n=1$ density, whereas the compressible superfluid is able to get to a much higher density. But then in that outer shell out towards the edges of the trap, those two lines start to overlap. From what we can measure about the density and what we know about the shape of the trap and therefore the chemical potential, we can extract the compressability and that looks as we expect as well. We see that the superfluid is compressible, somewhat insulating region in the centre of that trap being breathability drops down to zero. 
[pic - compressibility data]


OK, so that's one type of in situ image that you can make, where we are able to see some some density of atoms, but we can't, for instance, distinguish individual lattice sites. If we did want to distinguish individual lattice sites, we would just have to build an extremely high resolution imaging system. And so you know that the lattice spacing scales with the wavelength of the the light that forms the the lattice, so we're talking about getting a resolution here of 300-400 nanometres, potentially. Something else that you have to do if you want to make these single site measurements is you'll have to use fluorescence imaging in contrast with the absorption imaging techniques you've talked about previously. And so this means that you will illuminate your atomic sample with light, allow those atoms to fluoresce. As they fluoresce, they will start to heat up, so you need to simultaneously cool them. But one problem that you can't really get rid of is that while you're illuminating your sample to try to image them, you can create an excited molecular state. Two atoms can be excited to a molecular state, and lost from the image completely. And this has a fairly decent chance of happening, so you can actually be pretty sure that if there's any pair of atoms on the lattice site, they will be lost from your image. These are called light-assisted collisions, and this is why the quantum gas microscope is more of a parity measurement of the occupation of the lattice sites than a direct representation of the number of atoms that you have on each site. 

[images of parity measurements - not sure about keeping the verbal caption for it?]

We see a couple of different regimes here. Firstly, in this first column, it's a BEC superfluid and you see that this looks quite noisy in the reconstruction. And this makes sense when you think about the Poisson fluctuations that will exist on the actual atomic density within that superfluid mapped down onto that parity measurement. Things start to look a lot neater when we look at the Mott insulator, so on this, second column onwards. With a low atom number, the MI looks very nice, you can see that you have almost complete filling of that central region of your optical lattice. A few small defects, and you can see that they map quite nicely between the reconstructed and the raw data. But as you increase the atom number, so this Mott insulating region gets bigger and bigger, which you'd expect you're adding more atoms to the system, you get a few more defects appearing. And again, these are very nicely between the real and the reconstructed images. But then something quite interesting happens here, where you start to get this doughnut type shape where you've got this inner ring that looks empty. And this is where you have to remember that this is a parity measurement, that central region of the optical lattices is not empty. That's the $n=2$ Mott insulator, if you remember that wedding cake structure that you're seen last time for the Mott insulator. 

So this shows us that we can measure the insulating and superfluid states to really good accuracy, but also that we can prepare an initial Mott insulating state very, very well indeed. Whether we want that to be an $n=1,2$ we can control that. And you can do some quite nice things with this and this example here shows an experiment where the Mott insulating state is created and used to study quantum fluctuations. 
[slide: correlated fluctuations]
What we have here and this will be familiar from your treatment of the simulator, I think last time where you look at that,
when you look at the simulator using second order perturbation theory, where if you've got these small fluctuations an atom might be able to hop to a neighbouring site. And the picture that we have here is a little bit different to the ones that you've seen before because we have tunnelling possible along the $x$ direction, but no tunnelling possible along $y$, so instead of being a 2D array of lattice sites, it's more 1D chains that are all separated from each other. When this happens, you create neighbouring sites, one of which is empty and the neighbour has a double occupancy, and this is called the doublon-holon pair. And remember, the quantum gas microscope is a parity image, so this will show up as a pair of unoccupied sites. And you can see this in the real data, too, so. 
[pic- bottom left] 
Here we're going left to from Mott insulator, then reducing the lattice depth to go to a superfluid (the opposite way round from the way that it's normally shown). And you can see here that in this reconstruction, you very clearly have these gaps appearing in pairs, which is quite strong evidence of this quantum fluctuation process, whereas as you move towards superfluid, you get more noise and more different kinds of fluctuations on top of that, so those fluctuations are a little bit harder to distinguish. And if you want to look at this a little bit more quantitatively and you could look at the the correlation function between pairs of lattice sites. 
[eqn - top right of same slide + data underneath with big caption]
And you can plot that data, so in red is the correlations in the X direction and in blue, you see the correlations in the Y direction. Then you can create an increasingly sophisticated models that will capture the full physics of this, so the second curve there is a more sophisticated simulation, and then in the solid line again, you're incorporating the effects of the underlying harmonic confinement and also the finite temperature, which then starts to give you a much closer agreement to your data. So you can see that it's only in the small $U/J$ regime that that particular approach really captures what the experiments are doing. Which is, as you'd expect, for instance, if you have a finite temperature, you're going to start to see other effects that start to overshadow those quantum fluctuations. 

One final example to show before we pause for a moment, this is not examinable, it's very different to the examples we've seen before. I think most of the examples that seen before have been using optical lattices to study systems that look somewhat like lattices. But this is an example of using an optical lattice to reproduce some of the physics of the Higgs mode, which would normally be associated with this potential where you've got a ring shaped potential a flat ring. And this has two excitations associated with it; so got one mode along the flat ring, so that should be a gapless excitation, then you've got this Higgs mode. 
[pic]
And as you change the potential, those modes get softened out and this doesn't look anything an optical lattice. But optical lattices can have excitations, too. What we have here is an experimental sequence in an optical lattice system where a lattice is loaded and the depth of that lattice is modulated at some fixed frequency for a short duration. And what you see experimentally is that those excitations have some finite offset in frequency, so the data is run lots of times with lots of different modulation frequencies in the lattice. 
[pic] 
The key feature there is this offset in the frequency and this can be plotted. From SF towards the Mott insulator the frequency of that offset decreases, and this corresponds to the softening of those modes that we saw in the previous sketch. The other side is just the regular Mott insulator. We would expect the potential to be capped anyway, but we wouldn't necessarily expect that in the superfluid. This is just an indication of an optical lattice system that is being used to recreate the physics more traditionally associated with high energy physics.

Let's spend the next half hour or so talking about the thermodynamics of a cold atom system, more specifically an optical lattice system. And here we'll be trying to address the main question of how you actually define the temperature of a system of cold atoms in an optical lattice. So let's jump back to a comparison with solid state physics. In the solid state, you will have a system that is in contact with some experimental apparatus, so you can have heat exchange and particle exchange with some external reservoir. And so it makes sense to describe this in terms of the grand canonical ensemble where you have your chemical potential and your temperature defined by your environment. This is very different to cold atom systems, where we have our cold atoms in a vacuum chamber trapped in a laser lattice. And we can't have atoms in contact with the experimental apparatus. We actively avoid that. When you take the sequence step by step, there are a few different stages. Let's take evaporative cooling, which can be described as semi-open. You are evaporating away your high energy atoms, you are decreasing the atom number and you're decreasing the entropy of your system. That's the open stage of your cold atom experiment. And once you've loaded into a lattice, your system is closed and assuming that you've done everything right and you don't have terrible losses, your entropy and number should should stay constant. At first glance, it's very difficult to see how we would define the temperature because we don't have an environment, we don't have an external temperature to relate this to. 

let's look at that latest loading stage in a little bit more detail. The various parameters that we might be able to define and let's take them one by one and think about whether they are conserved or not during that loading process. 

\begin{itemize}
Energy is not conserved.
You can do work on your system.
The optical lattice system is a little bit more complicated, but if we just take the simple example of a harmonic oscillator,
we know that if we make that harmonic oscillator tighter or or weaker, we're going to change the separation between the energy levels.
We're changing the energy scales of our system. Likewise.
Temperature will also be affected. By the form of the trapping potential.
And. The are number assuming that you're doing things well and you're not you don't have losses and the outcome number should be conserved.
But once again, because the energy is changing, the chemical potential is not conserved.
That leaves us one quantity that we're pinning all of our hopes on to. And that is the entropy.
And the entropy can be conserved. 
\end{itemize}

In order to conserve entropy, ou need your transformation to be adiabatic, so you need it to be very, very slow. Let's let's assume that we've made a nice adiabatic transformation into our optical lattice; once we're there, by the second law thermodynamics, we can't decrease that entropy further. Whatever entropy we go into our optical lattice with, we can only make it worse. And this is always a conflict experimentally, because if you want to be adiabatic, you want to load as slowly as possible, but then that's going to conflict with the finite lifetime of the atoms in the trap and any heating or noise effects that you might have, which will heat the atoms up. But if you design your your sequence as carefully as possible, you can assume that the entropy is going to be roughly the same in the optical lattice as it was in the harmonic trap before lattice loading, or at least very similar. And so it's this knowledge of the entropy that we will use to define the temperature of the lattice. 

Let's think about how to do that, because there's a bit of a conceptual trick to this. If we just take a whole system, a whole closed system that would be described by the microcanonical ensemble where you're conserving $N, S, U$. But if we take out some small part of the system and just say this is some small sub system that is equilibrated with the rest of your system, so the the majority of the system acts a reservoir, and we just have this small subsystem, and then we can start to describe this in the grand canonical ensemble and define the chemical potential and the temperature. The only problem is we have no idea what the temperature is of the the main system that we're using as a reservoir. Let's split this up further into lots of smaller subsystems. And for each of these, we can measure locally the atom density as a function of $\mu, T$ and also the local entropy. And by integrating these over the entire system, we can then extract the atom number and entropy experimentally for that system, and then we can work out what combination of chemical potential and temperature are needed to obtain those measured values.
[eqn - solving self-consistently]
We can use that measurement then to get a temperature for our system. And this can be applied to inhomogeneous systems as well as homogeneous systems. And you might already spotted this trick of splitting everything into subsystems looks a little bit like the local density approximation that you learnt about previously, where you've defined your chemical potential locally as your central chemical potential with an offset determined by the potential that the underlying potential of the system. While your chemical potential will depend on the position in the trap, the temperature doesn't. 

So that is how we go about finding the temperature. But let's take a little digression into talking about entropy and how entropy is defined in a cold atom system. It's very convenient, or some would argue more correct, to work in terms of the von Neumann entropy, 
[eqn - S_{vn}]
which allows you to express the entropy in terms of your many body density matrix. And that's really handy because you can diagonalise it to give this very nice form, 
[eqn]
which allows you to get an intuitive picture of what's going on. To see that will take a couple of examples. The first is a pure state at zero temperature. $\rho_{11}=1$ and every other diagonal element will be zero. And so $S=0$, and this  makes sense when you think about the state as well, you would expect that entropy to be zero. 

In the opposite limit, e want to maximise our entropy now, and we do that by making our state as mixed as possible, so assigning an equal probability to any of the different possible states in our system, which then gives us a diagonal density matrix where elements will all be the same and they will all be $\rho_{ii}=1/N_H$ where $N_H$ is the dimensionality of our Hilbert space. This will maximise our entropy and gives $S=k_B \log(N_H)$ which is known as the \textbf{entropy capacity}, the greatest entropy that can be associated with that system. Just to put some typical experimental numbers to that. If we've got $10^5$ atoms in our system, our entropy roughly scale as Boltzmann constant times the number of atoms,
which gives us a very, very large and associated Hilbert space. This isn't a calculation that you would run by hand, but it does give a quite nice picture of what's going on. It's important to remember that this entropy isn't a direct observable of our system. And so we have to make some of the experimental measurements that we discussed before. 

OK, so that was our entropy, and let's quickly take a look at what the density matrix will look like for a thermal state.
[eqn]
This is purely thermal. Don't need to worry about Bose or Fermi statistics. We just need to worry about Boltzmann statistics, and so the density matrix elements will be scaled as $$\rho_{nn} = \frac{1}{Z}e^{-E_n/K_BT}$$. And it's important to remember that most density matrices that you could write down will not just describe a small state this because you require that Boltzmann type scaling. And an interesting question is raised here, which is that we know that time evolution in quantum mechanics is unitary, which implies if you start off with that perfect pure state that we saw in the last slide, that state should stay pure always. 

So let's look back at some measurements that can be made here. We've already said that if we want to measure the temperature or define the temperature of an optical lattice, we need to make some measurements of the number density and the entropy of the system and trace that back to find the temperature that you would expect would give those measured values. And we've got an example here. And as with all of these, it's been cut off from the slides, you have it in the lecture notes, and it's a good idea to look through some of these papers as well and repercussions and really digest what they're trying to show. What we have here is a comparison of the temperatures that are being extracted from an experiment with the temperatures that are the best fit from simulations. The top row here is the time of flight images as you increase the temperature of a cloud that's being lowered into the trap. 
[pic]
This looks a lot like the superfluid to insulator pictures that you've been looking at before. It's not. This is just superfluid turning into thermal state, and the temperatures that are written in the top panels are the temperatures that are that are associated with that state. The next row down is a quantum Monte Carlo simulation of of that system. Many, many of these simulations will have been run and the one that provides the best fit to the experimental data selected and said, OK, this is the one that describes our system. This is the temperature that we have simulated based on this data. And you can see that on this lower plot. This is just cut through the data and you can see that you have very good agreement even on some of these quite small scale features here, some small deviations. It's not completely perfect, but it's a fairly good description. And if you look at the temperature that's extracted from those simulations, it's a fairly close agreement with the temperature that was extracted experimentally. Not only does it nicely illustrate extracting a temperature from your measurements, it also gives some confidence in the assertions that the entropy is preserved fairly well, not perfectly, but fairly well during that loading process. 

This can all be studied, at the finite temperature now that we've known now that we know how to define the temperature. In the BH phase diagram you can see that we have some critical temperature associated with this  boundary between the superfluid and a normal fluid, and that will vary with this ratio $U/J$ until we reach our quantum critical point, which marks the phase transition between superfluid and Mott insulator. Yes, it is worth highlighting that that critical temperature will vanish at the quantum critical point. 
[pic, x2]
Also, you can you can plot this critical temperature directly as well and compare that with once again comparing it with the simulations, and you see that there's a fairly good agreement between the experimental data and the simulations for this process.


One final part of the main discussion here is once again looking at that quantum gas microscope, so back to single site resolved imaging and in our optical lattice. You can actually get a fairly good idea of the entropy of the system. 
[pic - plot of atom density in MI system as function of radial distance from centre]
The yellow line here is an $n=1$ Mott insulator so you can see the density, It's fairly stable at the density of one, you go into this superfluid shell and then have zero density exactly as you'd expect. The Red Line is an $n=2$ Mott insulator exactly as before parity measurements mean you have apparently zero density in centre is actually $n=2$ superfluid region. And you can also look at the fluctuations in that density, which gives a fairly good idea of the entropy of the system. And you can see that where the density is very low, you have very low fluctuations, as you would expect, but these fluctuations from a peak wherever the density goes to half filling, and that makes sense as well, because in this region is an equal probability of being either occupied or not occupied. The fluctuations there, we would expect will be very strong. And this is consistent between the $n=1$ and the $n=2$. This can be this radial distance can be mapped onto a chemical potential and exactly the same data plotted, so it's just taking this slice that you've seen before down the down the chemical potential.
[pic - bottom left]
And you can see that there's this very nice overlap, then between the data for the $n=1,2$ mott insulators. And once again, this peak in fluctuations at this hal filling region. We know that we're in the deep lattice limit, so we don't have correlations between a lot of sites and so that we know that the entropy that we're measuring is just a local entropy. And it is governed by this the statistics of the occupation of each site. While the entropy peaks at this half filling, the entropy is lowest or very low inside those insulating regions. So we can argue that the entropy is actively pushed away from the insulating regions into the superfluid regions, and this makes sense as well because we know that a system will want to minimise the energy cost associated with with the entropy, and in the Mott insulator there's a high energy cost associated with with putting entropy into that system, but in superfluid that energy cost is much lower, so all of the entropy will be pushed into the superfluid. This can cause heating of the superfluid shell, so it's actually a little bit debateable whether we should really be saying that these regions here are actually a superfluid or are they normal fluid because they've been heated by this entropy redistribution? 

One last quite cute example for the end (non-examinable), and it's quite exciting. This is work that is actually going on in some of the labs downstairs as we speak: trying to create negative absolute temperatures in an optical lattice. We've seen that in the optical lattice system we can use the entropy to find the temperature. Now let's look at what happens when we play around a bit with the energy and the entropy in the system. We have the relation
$$1/T=\partial S/\partial E$$
so increasing the energy of your system, you would expect to increase the entropy, increase the temperature. Let's look at this example here [left] where we've got a very low energy system, very low temperature, all of the atoms occupying the lowest energy levels in the system. And so the entropy is going to be very low indeed. This is all governed by Boltzmann statistics in a site.
[pic - evolution to show -T]
Let's add some more energy to our system, and higher and higher energy levels will start to be occupied. This will increase the entropy of our system. Again, all make sense here you are occupying those states, according to Boltzmann statistics. You just increasing the disorder in the system as you increase the temperature and increase the energy of the system. Now, this could go on forever, so let's put a lid on our system. And we say that we maximise the temperature when all of our states are equally occupied. Think about that exponential that requires a temperature of infinity. And we don't need to stop there. We carry on adding energy to our system, and see that we start to decrease our entropy once again, so exact mirror of what we had here. We're increasing our energy, preferentially occupying the states at to the higher energy levels of the system, decreasing our entropy again. Now, if we are decreasing our entropy by increasing our energy, \textbf{that implies that our temperature is negative}. And we can get to negative zero temperature again, and by increasing our energy so further until just the highest energy levels are occupied. This leads to some quite interesting consequences:

\begin{itemize}
Negative temperatures are actually hotter than positive temperatures,
if we put them in thermal context and heat is going to flow from the picture on the to the picture to the left of it.
our negative temperatures are hotter than positive temperatures and.

And we've got this strange discontinuity at the centre at this plus minus infinite temperature, and that one at least could be resolved when. And remember that typically, instead of working with temperature, you're actually working with beta, which is the inverse of temperature, so that resolves at least that strange discontinuity there. 

This isn't something that we see in everyday life. And that is because in everyday life, our energy scales are bounded from below. To achieve this, we need a Hamiltonian that is bounded from above. And this is what we get from band structure and knots, colitis or at least on the kinetic energy, so band structure provides, if we've got large enough band gap, our kinetic energy will be bounded from both above and below. 

\end{itemize}

But we have three terms in our BH Hamiltonian. Let's think about each of those because our interaction and potential energies are currently bounded from below. What we can do is increase our lattice strength so that we're ramping into a really deep Mott insulator. Atoms are pinned to individual lattice sites, kinetic energy already has bound from above and below, so we don't have to worry about that, our interaction and our potential energies we can reverse. Again, using a magnetic field to sweep the interaction energy, reversing the sign of the confinement to put an upper bound on the potential energy. Lower the lattice depth again. And our atoms are in a negative temperature state. This has been demonstrated experimentally in some lattices already in 2013, and this is experimental data 
[pic]
that shows in time of flight that a different region of the band structure is being occupied consistent with the stable occupation of all the negative temperatures state, and we're trying to do that right now in some more interesting lattices, so stay tuned to hear more about that.
